{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip5rKOFNZdfm"
   },
   "source": [
    "### Load libraries \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E8HabdXRZ6rK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "from IPython import display\n",
    "import datetime\n",
    "import numpy as np\n",
    "from skimage import util\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Dp-hbM_gZ6rW",
    "outputId": "c6645683-3c58-4426-9a44-0f32175435d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Education\\University\\4th Year\\ELEC6200 - Group Design Project\\gdp-wild-dogs\n"
     ]
    }
   ],
   "source": [
    "#!pwd\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking OS system \n",
    "- slight difference in file path for different OS systems:\n",
    "    - linux: '/home/user/Data'\n",
    "    - windows: 'C:\\\\user\\\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS System:  windows\n"
     ]
    }
   ],
   "source": [
    "os_system = sys.platform\n",
    "if os_system.startswith('win'):\n",
    "    os_system = 'windows'\n",
    "print('OS System: ', os_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Directory Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/\n",
      "    2020-11-05/\n",
      "        201105_807d3a2a0fe8_xyz.pkl\n",
      "        cut.pkl\n",
      "        cutoff.txt\n",
      "        dataFeatured.pkl\n",
      "        labelled.pkl\n",
      "        labels.csv\n",
      "        raw.pkl\n",
      "    2020-11-16/\n",
      "        201116_807d3a2a0fe8_xyz.pkl\n",
      "        cut.pkl\n",
      "        cutoff.txt\n",
      "        dataFeatured.pkl\n",
      "        labelled.pkl\n",
      "        labels.csv\n",
      "        raw.pkl\n",
      "    2020-11-23/\n",
      "        201123_807d3a2a0fe8_xyz.pkl\n",
      "        cut.pkl\n",
      "        cutoff.txt\n",
      "        dataFeatured.pkl\n",
      "        labelled.pkl\n",
      "        labels.csv\n",
      "        raw.pkl\n",
      "    2020-11-26/\n",
      "        201126_807d3a2a0fe8_xyz.pkl\n",
      "        cut.pkl\n",
      "        cutoff.txt\n",
      "        dataFeatured.pkl\n",
      "        labelled.pkl\n",
      "        labels.csv\n",
      "        raw.pkl\n",
      "    2020-11-27/\n",
      "        201127_807d3a2a0fe8_xyz.pkl\n",
      "        cut.pkl\n",
      "        cutoff.txt\n",
      "        dataFeatured.pkl\n",
      "        labelled.pkl\n",
      "        labels.csv\n",
      "        raw.pkl\n",
      "    2020-11-28/\n",
      "        201128_807d3a2a0fe8_xyz.pkl\n",
      "        raw.pkl\n",
      "    2020-12-03/\n",
      "        201203_807d3a2a0fe8.pkl\n",
      "        201203_807d3a2a0fe8_xyz.pkl\n",
      "        raw.pkl\n"
     ]
    }
   ],
   "source": [
    "list_files(os.path.join(os.getcwd(), 'Data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzFygJuEY6Wz"
   },
   "source": [
    " ### Functions to Trim, Load, and Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "q6zJuH2LZ6rk",
    "outputId": "156b2cf9-43e6-447d-db18-c085e763b23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS System:  windows\n"
     ]
    }
   ],
   "source": [
    "os_system = sys.platform\n",
    "if os_system.startswith('win'):\n",
    "    os_system = 'windows'\n",
    "print('OS System: ', os_system)\n",
    "\n",
    "if os_system == 'linux':\n",
    "    filepath = './Data'\n",
    "    pathstyle = '/'\n",
    "elif os_system == 'windows':\n",
    "    filepath = 'Data'\n",
    "    pathstyle = '\\\\'\n",
    "\n",
    "def trimData(folderPath):\n",
    "        \n",
    "    cutoffPresent = 0\n",
    "    \n",
    "    for filename in os.listdir(folderPath):\n",
    "        if filename == \"cutoff.txt\":\n",
    "            cutoffPresent = 1\n",
    "            f = open('./'+folderPath+'/cutoff.txt', \"r\")\n",
    "            start = f.readline()\n",
    "            start = datetime.time(int(start.split(\",\")[0]), int(start.split(\",\")[1]), int(start.split(\",\")[2]))\n",
    "            end = f.readline()\n",
    "            end = datetime.time(int(end.split(\",\")[0]), int(end.split(\",\")[1]), int(end.split(\",\")[2]))\n",
    "    for filename in os.listdir(folderPath):\n",
    "        if cutoffPresent == 1:\n",
    "            if filename == \"raw.pkl\":\n",
    "                df = pd.read_pickle(subdir+pathstyle+ filename)\n",
    "                df.reset_index(inplace=True)\n",
    "                df = df.drop(['mag_x', 'mag_y', 'mag_z', 'pressure'], axis=1)\n",
    "                display.display(df)\n",
    "                df = df.loc[(df['dt'].dt.time > start) & (df['dt'].dt.time < end)]\n",
    "                df.to_pickle(\"./\"+folderPath+\"/cut.pkl\")\n",
    "                cutoffPresent = 0\n",
    "    return 0\n",
    "\n",
    "def loadData(folderPath):\n",
    "    for filename in os.listdir(folderPath):\n",
    "        if filename == \"cut.pkl\":\n",
    "            data = pd.read_pickle(folderPath+pathstyle+filename)\n",
    "            data.reset_index(inplace=True)\n",
    "            return data\n",
    "\n",
    "def loadLabels(folderPath):\n",
    "    for filename in os.listdir(folderPath):\n",
    "        if filename == \"labels.csv\":\n",
    "            labels = pd.read_csv(folderPath+pathstyle+filename)\n",
    "            labels.reset_index(inplace=True)\n",
    "            return labels\n",
    "\n",
    "def labelData(data, labels):\n",
    "    if('activity' in data.columns):\n",
    "                    data.drop('activity', 1, inplace=True)\n",
    "\n",
    "    labelsList = []\n",
    "    intervalIndex = 0\n",
    "    intervalBegin = pd.to_datetime(labels['start_time'][intervalIndex], utc=True)\n",
    "    intervalEnd = pd.to_datetime(labels['end_time'][intervalIndex], utc=True)\n",
    "    for i in range(0, len(data)):\n",
    "        time = pd.to_datetime(data['dt'][i])\n",
    "        while(time > intervalEnd):\n",
    "            if(intervalIndex + 1 >= len(labels)):\n",
    "                break\n",
    "            intervalIndex += 1\n",
    "            intervalBegin = pd.to_datetime(labels['start_time'][intervalIndex], utc=True)\n",
    "            intervalEnd = pd.to_datetime(labels['end_time'][intervalIndex], utc=True)\n",
    "        if(time > intervalEnd or time < intervalBegin):\n",
    "            labelsList.append(\"None\")\n",
    "        else:\n",
    "            labelsList.append(labels[\"activity\"][intervalIndex])\n",
    "    data[\"activity\"] = labelsList\n",
    "\n",
    "def loadAndLabel():\n",
    "        \n",
    "    labelledDfList = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(filepath):\n",
    "        if subdir != \"Data\":\n",
    "            data = loadData(subdir)\n",
    "            labels = loadLabels(subdir)\n",
    "            if (data is not None) & (labels is not None):\n",
    "                print(\"Labelled Data from {}\".format((str(pd.to_datetime(data['dt'][0], utc=True))[0:10])))\n",
    "                labelData(data, labels)\n",
    "                labelledDfList.append(data)\n",
    "                \n",
    "    return labelledDfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled Data from 2020-11-05\n",
      "Labelled Data from 2020-11-16\n",
      "Labelled Data from 2020-11-23\n",
      "Labelled Data from 2020-11-26\n",
      "Labelled Data from 2020-11-27\n",
      "51603\n"
     ]
    }
   ],
   "source": [
    "dfList = loadAndLabel()\n",
    "print(len(dfList[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DF 0\n",
      "Starting DF 1\n",
      "Starting DF 2\n",
      "Starting DF 3\n",
      "Starting DF 4\n"
     ]
    }
   ],
   "source": [
    "query_variables = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "composite_variables = []\n",
    "windowSize = 10 #Maximum Size of window on either side of point\n",
    "dfIndex = 0;\n",
    "show_progress=False\n",
    "for df in dfList:\n",
    "    print(\"Starting DF \" + str(dfIndex))\n",
    "    values = {}\n",
    "    for variable in query_variables:\n",
    "            values[variable + \"_MEAN\"] = []\n",
    "            values[variable + \"_VAR\"] = []\n",
    "    for i in range(0, len(df.index)):\n",
    "        minIndex = 0\n",
    "        maxIndex = len(df.index)\n",
    "        if(i >= windowSize):\n",
    "            minIndex = i - windowSize\n",
    "        if(i <= len(df.index)-1-windowSize):\n",
    "            maxIndex = i+windowSize\n",
    "        slice = df.iloc[minIndex:maxIndex+1]\n",
    "        for variable in query_variables:\n",
    "            values[variable + \"_MEAN\"].append(slice[variable].mean())\n",
    "            values[variable + \"_VAR\"].append(slice[variable].var())\n",
    "        if(show_progress):\n",
    "            display.clear_output()\n",
    "            print(\"Processing Dataframe: \" + str(dfIndex))\n",
    "            print(\"Percentage Finished: \" + str(float(i)*100/len(df.index)) + \"%\")\n",
    "    for variable in query_variables:\n",
    "            df[variable + \"_MEAN\"] = values[variable + \"_MEAN\"]\n",
    "            df[variable + \"_VAR\"] = values[variable + \"_VAR\"]\n",
    "            if(not (variable + \"_MEAN\") in composite_variables):\n",
    "                composite_variables.append((variable + \"_MEAN\"))\n",
    "            if(not (variable + \"_VAR\") in composite_variables):\n",
    "                composite_variables.append((variable + \"_VAR\"))\n",
    "    dfIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reciprocal_column(df, column1, column2, dfIndex, log=False, verbose=False):\n",
    "    count = 0\n",
    "    if(column1 in df.columns and column2 in df.columns):\n",
    "        print(\"Forming reciprocal of \" + column1 + \"/\" + column2)\n",
    "        reciprocal_vals = []\n",
    "        if((column1 + \"/\" + column2) in df.columns):\n",
    "            df.drop(column1 + \"/\" + column2, 1, inplace=True)\n",
    "        \n",
    "        new_df = df\n",
    "        for i in range(len(df)):\n",
    "            x = df[column1][i]\n",
    "            y = df[column2][i]\n",
    "            if(x == 0):\n",
    "                x = 0.00001\n",
    "            if(y == 0):\n",
    "                y = 0.00001\n",
    "            \n",
    "            result = x/y\n",
    "            if(np.isnan(result) or np.isinf(result)):\n",
    "                count += 1\n",
    "                result = 0.00001\n",
    "            if(log):\n",
    "                result = np.log10(np.abs(result))\n",
    "            reciprocal_vals.append(result)\n",
    "            \n",
    "            if(verbose):\n",
    "                display.clear_output()\n",
    "                print(\"Forming reciprocal of \" + column1 + \"/\" + column2 + \" for Dataframe \" + str(dfIndex))\n",
    "                print(\"Percentage Finished: \" + str(float(i)*100/len(df)) + \"%\")\n",
    "                print(\"NaN or Inf results: \" + str(count))\n",
    "        new_df[column1 + \"/\" + column2] = reciprocal_vals\n",
    "        \n",
    "        return new_df\n",
    "    else:\n",
    "        print(\"Columns not present in Dataframe. Cannot form Reciprocal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming reciprocal of acc_x/acc_y\n",
      "Forming reciprocal of acc_x/acc_y\n",
      "Forming reciprocal of acc_x/acc_y\n",
      "Forming reciprocal of acc_x/acc_y\n",
      "Forming reciprocal of acc_x/acc_y\n",
      "Forming reciprocal of acc_x/acc_z\n",
      "Forming reciprocal of acc_x/acc_z\n",
      "Forming reciprocal of acc_x/acc_z\n",
      "Forming reciprocal of acc_x/acc_z\n",
      "Forming reciprocal of acc_x/acc_z\n",
      "Forming reciprocal of acc_y/acc_z\n",
      "Forming reciprocal of acc_y/acc_z\n",
      "Forming reciprocal of acc_y/acc_z\n",
      "Forming reciprocal of acc_y/acc_z\n",
      "Forming reciprocal of acc_y/acc_z\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(dfList)):\n",
    "    dfList[i] = add_reciprocal_column(dfList[i], \"acc_x\", \"acc_y\", i, log=True)\n",
    "for i in range(0,len(dfList)):\n",
    "    dfList[i] = add_reciprocal_column(dfList[i], \"acc_x\", \"acc_z\", i, log=True)\n",
    "for i in range(0,len(dfList)):\n",
    "    dfList[i] = add_reciprocal_column(dfList[i], \"acc_y\", \"acc_z\", i, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fourier(column, df, rate, label, use_label=False, M = 64, freq_limit=-1, show=False):\n",
    "    if(column in df.columns):\n",
    "        data = np.array(df[column])\n",
    "        if(use_label):\n",
    "            data = np.array(df.loc[df['activity'] == label][column])\n",
    "        N = data.shape[0]\n",
    "        if(N <= M):\n",
    "            if(use_label):\n",
    "                print(\"Insufficient Data for activity: \" + label)\n",
    "            else:\n",
    "                print(\"Insufficient Complete Data\")\n",
    "            return []\n",
    "        L = N / rate\n",
    "        slices = util.view_as_windows(data, window_shape=(M,), step=1)\n",
    "        slices = slices * np.hanning(M + 1)[:-1]\n",
    "        slices = slices.T\n",
    "        spectrum = np.fft.fft(slices, axis=0)[:M // 2 + 1:-1]\n",
    "        spectrum = np.abs(spectrum)\n",
    "        \n",
    "\n",
    "        S = np.abs(spectrum)\n",
    "        #S = 20 * np.log10(S / np.max(S))\n",
    "        S = S / np.max(S)\n",
    "\n",
    "        if(show):\n",
    "            f, ax = plt.subplots(figsize=(20, 10))\n",
    "            if(freq_limit != -1):\n",
    "                ax.imshow(S[0:freq_limit], origin='lower', cmap='viridis', extent=(0, L, 0, freq_limit))\n",
    "            else:\n",
    "                ax.imshow(S, origin='lower', cmap='viridis', extent=(0, L, 0, np.max(np.fft.fftfreq(M, d=1/rate))))\n",
    "            ax.axis('tight')\n",
    "            y_label = 'Frequency [Hz] of value: '\n",
    "            y_label = y_label + column\n",
    "            ax.set_ylabel(y_label)\n",
    "            ax.set_xlabel(label + ' Time [s]');\n",
    "        #S has first index referencing frequency in Hz, second index is window index\n",
    "        #print(L)\n",
    "        return S\n",
    "    \n",
    "def add_fourier_variables(variable_frequencies, labelledDFList, window_size):\n",
    "    fourier_variables = []\n",
    "    for df in labelledDFList:\n",
    "        for variable in variable_frequencies:\n",
    "            frequency_indices = variable_frequencies[variable]\n",
    "            S = calculate_fourier(variable, df, 50, '', use_label=False, M=window_size, show=False)\n",
    "            for frequency_index in frequency_indices:\n",
    "                new_column = []\n",
    "                new_column.extend(np.zeros(window_size-1))\n",
    "                new_column.extend(S[frequency_index])\n",
    "                print(variable + \" at frequency index: \" + str(frequency_index))\n",
    "                new_var_name = variable + \"_freq_\" + str(frequency_index)\n",
    "                if(new_var_name in df.columns):\n",
    "                    df.drop(new_var_name, 1, inplace=True)\n",
    "                df[new_var_name] = new_column\n",
    "                if(not new_var_name in fourier_variables):\n",
    "                    fourier_variables.append(new_var_name)\n",
    "    return fourier_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "def split_test_train(data, training_variables, excluded_activities=[], test_ratio=0.25, perform_one_hot=False, undersample=True, oversample=False):\n",
    "    excluded_activities.append('None')\n",
    "    new_data = data[~data['activity'].isin(excluded_activities)]\n",
    "    activities = new_data['activity'].unique()\n",
    "    print(\"Activity Data Count:\")\n",
    "    print(new_data['activity'].value_counts())\n",
    "    print()\n",
    "    if(undersample):\n",
    "        print(\"Undersampling to: \" + str(np.min(new_data['activity'].value_counts())) + \" data points per activity\" )\n",
    "    elif(oversample):\n",
    "        print(\"Oversampling to: \" + str(np.max(new_data['activity'].value_counts())) + \" data points per activity\" )\n",
    "    independent_data = new_data[training_variables]\n",
    "    \n",
    "    one_hot=new_data['activity'].to_numpy()\n",
    "    print(one_hot)\n",
    "    if(perform_one_hot):\n",
    "        one_hot = pd.get_dummies(new_data['activity'])\n",
    "        if(undersample):\n",
    "            rus = RandomUnderSampler()\n",
    "            X_resampled, y_resampled = rus.fit_resample(independent_data.to_numpy(), one_hot.to_numpy())\n",
    "            #Returns X_train,X_test,y_train,y_test, independent_data.columns, one_hot.columns\n",
    "            return (train_test_split(X_resampled,y_resampled,test_size=test_ratio), independent_data.columns, one_hot.columns)\n",
    "        elif(oversample):\n",
    "            ros = RandomOverSampler()\n",
    "            X_resampled, y_resampled = ros.fit_resample(independent_data.to_numpy(), one_hot)\n",
    "            #Returns X_train,X_test,y_train,y_test, independent_data.columns, one_hot.columns\n",
    "            return (train_test_split(X_resampled,y_resampled,test_size=test_ratio), independent_data.columns, one_hot.columns)\n",
    "        else:\n",
    "            return (train_test_split(independent_data.to_numpy(),one_hot.to_numpy(),test_size=test_ratio), independent_data.columns, one_hot.columns)\n",
    "    else:\n",
    "        one_hot=new_data['activity'].to_numpy()\n",
    "        if(undersample):\n",
    "            rus = RandomUnderSampler()\n",
    "            X_resampled, y_resampled = rus.fit_resample(independent_data.to_numpy(), one_hot)\n",
    "            #Returns X_train,X_test,y_train,y_test, independent_data.columns, one_hot.columns\n",
    "            return (train_test_split(X_resampled,y_resampled,test_size=test_ratio), independent_data.columns, [])\n",
    "        elif(oversample):\n",
    "            ros = RandomOverSampler()\n",
    "            X_resampled, y_resampled = ros.fit_resample(independent_data.to_numpy(), one_hot)\n",
    "            #Returns X_train,X_test,y_train,y_test, independent_data.columns, one_hot.columns\n",
    "            return (train_test_split(X_resampled,y_resampled,test_size=test_ratio), independent_data.columns, [])\n",
    "        else:\n",
    "            return (train_test_split(independent_data.to_numpy(),one_hot,test_size=test_ratio), independent_data.columns, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_x at frequency index: 0\n",
      "acc_x at frequency index: 2\n",
      "acc_x at frequency index: 3\n",
      "acc_x at frequency index: 4\n",
      "acc_x at frequency index: 5\n",
      "acc_x at frequency index: 7\n",
      "acc_y at frequency index: 0\n",
      "acc_y at frequency index: 4\n",
      "acc_y at frequency index: 5\n",
      "acc_z at frequency index: 0\n",
      "acc_z at frequency index: 2\n",
      "acc_z at frequency index: 3\n",
      "acc_z at frequency index: 4\n",
      "acc_z at frequency index: 5\n",
      "gyro_x at frequency index: 5\n",
      "gyro_y at frequency index: 3\n",
      "gyro_y at frequency index: 4\n",
      "acc_x/acc_y at frequency index: 0\n",
      "acc_x/acc_y at frequency index: 2\n",
      "acc_x/acc_y at frequency index: 3\n",
      "acc_x/acc_y at frequency index: 4\n",
      "acc_x/acc_z at frequency index: 5\n",
      "acc_y/acc_z at frequency index: 0\n",
      "acc_y/acc_z at frequency index: 2\n",
      "acc_y/acc_z at frequency index: 3\n",
      "acc_x at frequency index: 0\n",
      "acc_x at frequency index: 2\n",
      "acc_x at frequency index: 3\n",
      "acc_x at frequency index: 4\n",
      "acc_x at frequency index: 5\n",
      "acc_x at frequency index: 7\n",
      "acc_y at frequency index: 0\n",
      "acc_y at frequency index: 4\n",
      "acc_y at frequency index: 5\n",
      "acc_z at frequency index: 0\n",
      "acc_z at frequency index: 2\n",
      "acc_z at frequency index: 3\n",
      "acc_z at frequency index: 4\n",
      "acc_z at frequency index: 5\n",
      "gyro_x at frequency index: 5\n",
      "gyro_y at frequency index: 3\n",
      "gyro_y at frequency index: 4\n",
      "acc_x/acc_y at frequency index: 0\n",
      "acc_x/acc_y at frequency index: 2\n",
      "acc_x/acc_y at frequency index: 3\n",
      "acc_x/acc_y at frequency index: 4\n",
      "acc_x/acc_z at frequency index: 5\n",
      "acc_y/acc_z at frequency index: 0\n",
      "acc_y/acc_z at frequency index: 2\n",
      "acc_y/acc_z at frequency index: 3\n",
      "acc_x at frequency index: 0\n",
      "acc_x at frequency index: 2\n",
      "acc_x at frequency index: 3\n",
      "acc_x at frequency index: 4\n",
      "acc_x at frequency index: 5\n",
      "acc_x at frequency index: 7\n",
      "acc_y at frequency index: 0\n",
      "acc_y at frequency index: 4\n",
      "acc_y at frequency index: 5\n",
      "acc_z at frequency index: 0\n",
      "acc_z at frequency index: 2\n",
      "acc_z at frequency index: 3\n",
      "acc_z at frequency index: 4\n",
      "acc_z at frequency index: 5\n",
      "gyro_x at frequency index: 5\n",
      "gyro_y at frequency index: 3\n",
      "gyro_y at frequency index: 4\n",
      "acc_x/acc_y at frequency index: 0\n",
      "acc_x/acc_y at frequency index: 2\n",
      "acc_x/acc_y at frequency index: 3\n",
      "acc_x/acc_y at frequency index: 4\n",
      "acc_x/acc_z at frequency index: 5\n",
      "acc_y/acc_z at frequency index: 0\n",
      "acc_y/acc_z at frequency index: 2\n",
      "acc_y/acc_z at frequency index: 3\n",
      "acc_x at frequency index: 0\n",
      "acc_x at frequency index: 2\n",
      "acc_x at frequency index: 3\n",
      "acc_x at frequency index: 4\n",
      "acc_x at frequency index: 5\n",
      "acc_x at frequency index: 7\n",
      "acc_y at frequency index: 0\n",
      "acc_y at frequency index: 4\n",
      "acc_y at frequency index: 5\n",
      "acc_z at frequency index: 0\n",
      "acc_z at frequency index: 2\n",
      "acc_z at frequency index: 3\n",
      "acc_z at frequency index: 4\n",
      "acc_z at frequency index: 5\n",
      "gyro_x at frequency index: 5\n",
      "gyro_y at frequency index: 3\n",
      "gyro_y at frequency index: 4\n",
      "acc_x/acc_y at frequency index: 0\n",
      "acc_x/acc_y at frequency index: 2\n",
      "acc_x/acc_y at frequency index: 3\n",
      "acc_x/acc_y at frequency index: 4\n",
      "acc_x/acc_z at frequency index: 5\n",
      "acc_y/acc_z at frequency index: 0\n",
      "acc_y/acc_z at frequency index: 2\n",
      "acc_y/acc_z at frequency index: 3\n",
      "acc_x at frequency index: 0\n",
      "acc_x at frequency index: 2\n",
      "acc_x at frequency index: 3\n",
      "acc_x at frequency index: 4\n",
      "acc_x at frequency index: 5\n",
      "acc_x at frequency index: 7\n",
      "acc_y at frequency index: 0\n",
      "acc_y at frequency index: 4\n",
      "acc_y at frequency index: 5\n",
      "acc_z at frequency index: 0\n",
      "acc_z at frequency index: 2\n",
      "acc_z at frequency index: 3\n",
      "acc_z at frequency index: 4\n",
      "acc_z at frequency index: 5\n",
      "gyro_x at frequency index: 5\n",
      "gyro_y at frequency index: 3\n",
      "gyro_y at frequency index: 4\n",
      "acc_x/acc_y at frequency index: 0\n",
      "acc_x/acc_y at frequency index: 2\n",
      "acc_x/acc_y at frequency index: 3\n",
      "acc_x/acc_y at frequency index: 4\n",
      "acc_x/acc_z at frequency index: 5\n",
      "acc_y/acc_z at frequency index: 0\n",
      "acc_y/acc_z at frequency index: 2\n",
      "acc_y/acc_z at frequency index: 3\n"
     ]
    }
   ],
   "source": [
    "useful_frequencies = {'acc_x': [0, 2, 3, 4, 5, 7], 'acc_y': [0, 4, 5], 'acc_z': [0, 2, 3, 4, 5], 'gyro_x': [5], 'gyro_y': [3, 4], 'acc_x/acc_y': [0, 2, 3, 4], 'acc_x/acc_z': [5], 'acc_y/acc_z': [0, 2, 3]}\n",
    "fourier_vars = add_fourier_variables(useful_frequencies, dfList, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "useful_values = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"acc_x/acc_y\", \"acc_x/acc_z\", \"acc_y/acc_z\"] + composite_variables + fourier_vars\n",
    "activities_to_exclude = ['barking', 'jumping', 'None']#, 'eating', 'jumping', 'playing', 'standing']\n",
    "dfconcat = pd.concat(dfList)\n",
    "dfconcat = dfconcat[~dfconcat['activity'].isin(activities_to_exclude)]\n",
    "activities = dfconcat['activity'].unique()\n",
    "\n",
    "\n",
    "independent_data = dfconcat[useful_values].to_numpy()\n",
    "labels = dfconcat['activity'].to_numpy()\n",
    "    \n",
    "\n",
    "train = pd.concat(dfList[0:3])\n",
    "train = train[~train['activity'].isin(activities_to_exclude)]\n",
    "trainx = train[useful_values].to_numpy()\n",
    "trainy = train['activity'].to_numpy()\n",
    "\n",
    "test_temp = dfList[4]\n",
    "test_temp = test_temp[~test_temp['activity'].isin(activities_to_exclude)]\n",
    "test_tempx = test_temp[useful_values].to_numpy()\n",
    "test_tempy = test_temp['activity'].to_numpy()\n",
    "\n",
    "pseudo_trainx, testx, pseudo_trainy, testy = train_test_split(test_tempx, test_tempy, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.46754692163708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#trees_classifier = ExtraTreesClassifier(n_estimators=100, criterion='entropy')\n",
    "#trees_classifier.fit(trainx,trainy)\n",
    "#predictions = trees_classifier.predict(pseudo_trainx)\n",
    "\n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators = 100)\n",
    "adaboost_classifier.fit(trainx,trainy)\n",
    "predictions = adaboost_classifier.predict(pseudo_trainx)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for x in range(0, len(predictions)):\n",
    "    if predictions[x] == pseudo_trainy[x]:\n",
    "        correct += 1\n",
    "        \n",
    "print(correct/len(predictions)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2x = np.append(trainx, pseudo_trainx, axis=0)\n",
    "train2y = np.append(trainy, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.09318561238427\n"
     ]
    }
   ],
   "source": [
    "adaboost_classifier.fit(train2x, train2y)\n",
    "predictions2 = adaboost_classifier.predict(testx)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for x in range(0, len(predictions2)):\n",
    "    if predictions2[x] == testy[x]:\n",
    "        correct += 1\n",
    "        \n",
    "print(correct/len(predictions2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "szxxSBIuZF5W"
   ],
   "name": "gdp_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
